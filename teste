# Initialize the Random Forest classifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)

# Define the parameter grid to search
param_distributions = {
    'n_estimators': np.arange(100, 501, 50),  # Number of trees in the forest
    'max_depth': [None] + list(np.arange(5, 51, 5)),  # Maximum depth of the tree, including 'None' for unlimited depth
    'min_samples_split': np.arange(2, 21, 2),  # Minimum number of samples required to split an internal node
    'min_samples_leaf': np.arange(1, 21, 2),  # Minimum number of samples required to be at a leaf node
    'max_features': ['auto', 'sqrt', 'log2']  # The number of features to consider when looking for the best split
}

# Initialize the RandomizedSearchCV object
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)

# Fit the random search model
random_search.fit(X_train, y_train)

# Print the best parameters and the best score achieved
print(f'Best Parameters: {random_search.best_params_}')
print(f'Best Cross-Validation Score: {random_search.best_score_:.2f}')

# Use the best estimator to make predictions on the test set
best_rf = random_search.best_estimator_
y_pred = best_rf.predict(X_test)

# Calculate and print the accuracy on the test set
accuracy = accuracy_score(y_test, y_pred)
print(f'Test Set Accuracy: {accuracy:.2f}')







param_distributions = {
    'n_estimators': np.arange(100, 1001, 100),  # Number of trees, exploring a wide range from 100 to 1000
    'max_depth': [None] + list(np.arange(5, 51, 5)),  # Maximum depth, including 'None' for unlimited depth and increments of 5 up to 50
    'min_samples_split': [2, 5, 10, 15, 20],  # Minimum number of samples required to split a node, starting from 2 (default) and including other small values
    'min_samples_leaf': [1, 2, 4, 6, 8, 10],  # Minimum number of samples required at a leaf node, starting from 1 (default) and exploring slightly larger values
    'max_features': ['auto', 'sqrt', 'log2', None] + list(np.linspace(0.1, 1.0, 10, endpoint=True)),  # The number of features to consider when looking for the best split, including 'auto', 'sqrt', 'log2', 'None' (all features), and a range from 10% to 100% of the features
    'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree
    'max_samples': [None] + list(np.linspace(0.1, 1.0, 10, endpoint=True))  # The fraction of samples to be used for fitting each individual base learner. If None (default), the whole dataset is used to build each tree
}
