import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.metrics import accuracy_score

X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


rf = RandomForestClassifier(n_estimators=100, random_state=42)
xgboost = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')


stacking_clf = StackingClassifier(estimators=[('rf', rf), ('xgb', xgboost)], final_estimator=LogisticRegression(), cv=5)


stacking_clf = StackingClassifier(estimators=[('rf', rf), ('xgb', xgboost)], final_estimator=LogisticRegression(), cv=5)



from sklearn.inspection import permutation_importance

# Perform permutation importance
results = permutation_importance(stacking_clf, X_test, y_test, n_repeats=10, random_state=42, scoring='accuracy')

# Get the importance of each feature
feature_importance = results.importances_mean

# Print the feature importances
for i, importance in enumerate(feature_importance):
    print(f'Feature {i}: {importance}')
