import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW

# Custom dataset class
class CustomDataset(Dataset):
    def __init__(self, text_list, tokenizer, max_length=50):
        self.input_ids = []
        self.tokenizer = tokenizer
        for text in text_list:
            tokenized_text = tokenizer.encode(text, max_length=max_length, return_tensors='pt')
            self.input_ids.append(tokenized_text)
        
    def __len__(self):
        return len(self.input_ids)
    
    def __getitem__(self, idx):
        return {'input_ids': self.input_ids[idx].squeeze(0)}

# Sample dataset
sample_text_dataset = [
    "Once upon a time in a faraway land...",
    "In a galaxy far, far away...",
    "John works as a software engineer at XYZ Company.",
    "Roses are red, violets are blue...",
]

# Load pre-trained GPT-2 model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2', config=GPT2Config.from_pretrained('gpt2'))

# Prepare the dataset
custom_dataset = CustomDataset(sample_text_dataset, tokenizer)
dataloader = DataLoader(custom_dataset, batch_size=2, shuffle=True)

# Training loop
optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch in dataloader:
        input_ids = batch['input_ids']
        labels = input_ids.clone()
        
        optimizer.zero_grad()
        
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Save the trained model
model.save_pretrained("gpt2_finetuned_custom_model")

# Inference
def generate_answer(prompt, model, tokenizer):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

# Example usage
prompt = "Once upon a time"
generated_answer = generate_answer(prompt, model, tokenizer)
print(generated_answer)



----


import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW

# Custom dataset class
class CustomDataset(Dataset):
    def __init__(self, text_list, tokenizer, max_length=50):
        self.input_ids = []
        self.tokenizer = tokenizer
        for text in text_list:
            tokenized_text = tokenizer.encode(text, max_length=max_length, return_tensors='pt')
            self.input_ids.append(tokenized_text)
        
    def __len__(self):
        return len(self.input_ids)
    
    def __getitem__(self, idx):
        return {'input_ids': self.input_ids[idx].squeeze(0)}

# Sample dataset
sample_text_dataset = [
    "Once upon a time in a faraway land...",
    "In a galaxy far, far away...",
    "John works as a software engineer at XYZ Company.",
    "Roses are red, violets are blue...",
]

# Load pre-trained GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2', config=GPT2Config.from_pretrained('gpt2'))
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Prepare the dataset
custom_dataset = CustomDataset(sample_text_dataset, tokenizer)
dataloader = DataLoader(custom_dataset, batch_size=2, shuffle=True)

# Training loop
optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch in dataloader:
        input_ids = batch['input_ids']
        labels = input_ids.clone()
        
        optimizer.zero_grad()
        
        outputs = model(input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

# Save the trained model
model.save_pretrained("gpt2_finetuned_custom_model")

# Inference
def generate_answer(prompt, model, tokenizer):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

---

from transformers.tokenization_openai import OpenAIGPTTokenizer
from transformers.modeling_tf_openai import TFOpenAIGPTLMHeadModel
model = TFOpenAIGPTLMHeadModel.from_pretrained("openai-gpt")
tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")

# Example usage
prompt = "Once upon a time"
generated_answer = generate_answer(prompt, model, tokenizer)
print(generated_answer)
